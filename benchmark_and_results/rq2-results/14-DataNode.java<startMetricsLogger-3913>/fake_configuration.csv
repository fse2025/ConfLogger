,name,value,description
0,dfs.datanode.metrics.logger.period.seconds,-100,"This setting controls how frequently the DataNode logs its metrics. The
    logging configuration must also define one or more appenders for
    DataNodeMetricsLog for the metrics to be logged.
    DataNode metrics logging is disabled if this value is set to zero or
    less than zero."
1,dfs.namenode.delegation.key.update-interval,86400000,"The update interval for master key for delegation tokens 
       in the namenode in milliseconds."
2,yarn.nodemanager.localizer.fetch.thread-count,4,
3,mapreduce.job.counters.max,120,The max number of user counters allowed per job.
4,ipc.[port_number].weighted-cost.response,1,"The weight multiplier to apply to the time spent in the
    RESPONSE phase which do not involve holding a lock.
    See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on
    this phase. This property applies to WeightedTimeCostProvider."
5,dfs.client.read.shortcircuit.streams.cache.size,256,"The DFSClient maintains a cache of recently opened file descriptors.
    This parameter controls the maximum number of file descriptors in the cache.
    Setting this higher will use more file descriptors,
    but potentially provide better performance on workloads
    involving lots of seeks."
6,fs.trash.interval,0,"Number of minutes after which the checkpoint
  gets deleted.  If zero, the trash feature is disabled.
  This option may be configured both on the server and the
  client. If trash is disabled server side then the client
  side configuration is checked. If trash is enabled on the
  server side then the value configured on the server is
  used and the client configuration value is ignored."
7,yarn.timeline-service.writer.async.queue.capacity,100,
8,yarn.nodemanager.node-labels.resync-interval-ms,120000,
9,mapreduce.map.maxattempts,4,"Expert: The maximum number of attempts per map task.
  In other words, framework will try to execute a map task these many number
  of times before giving up on it."
