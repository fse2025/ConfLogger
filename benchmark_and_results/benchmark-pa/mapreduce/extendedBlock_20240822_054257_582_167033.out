====================TaintedSinkInfo:=======================
-------------TaintedSinkInfo----------
fileName:	JobResourceUploader.java	methodSinagture:	org.apache.hadoop.mapreduce.JobResourceUploader.addLog4jToDistributedCache(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V	methodLines:	794:802
blockLines:	798:-1
paras:	mapreduce.job.log4j-properties-file
TaintedStat:	NORMAL addLog4jToDistributedCache:conditional branch(ne, to iindex=23) 12,13 Node: < Application, Lorg/apache/hadoop/mapreduce/JobResourceUploader, addLog4jToDistributedCache(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V > Context: Everywhere
Source:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/JobResourceUploader, addLog4jToDistributedCache(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V > Context: Everywhere[6]10 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, get(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String; > 6,7,8 @10 exception:9
Tainted Path:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/JobResourceUploader, addLog4jToDistributedCache(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V > Context: Everywhere[6]10 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, get(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String; > 6,7,8 @10 exception:9
NORMAL addLog4jToDistributedCache:12 = invokevirtual < Application, Ljava/lang/String, isEmpty()Z > 10 @17 exception:11 Node: < Application, Lorg/apache/hadoop/mapreduce/JobResourceUploader, addLog4jToDistributedCache(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V > Context: Everywhere
METHOD_ENTRY:Node: < Primordial, Ljava/lang/String, isEmpty()Z > Context: Everywhere
NORMAL isEmpty:return 7 Node: < Primordial, Ljava/lang/String, isEmpty()Z > Context: Everywhere
NORMAL_RET_CALLEE:Node: < Primordial, Ljava/lang/String, isEmpty()Z > Context: Everywhere
NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/JobResourceUploader, addLog4jToDistributedCache(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V > Context: Everywhere[9]12 = invokevirtual < Application, Ljava/lang/String, isEmpty()Z > 10 @17 exception:11
NORMAL addLog4jToDistributedCache:conditional branch(ne, to iindex=23) 12,13 Node: < Application, Lorg/apache/hadoop/mapreduce/JobResourceUploader, addLog4jToDistributedCache(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V > Context: Everywhere



====================ExtendedBlocks:=======================


====================MethodSrc:=======================
      throws IOException {
    Configuration conf = job.getConfiguration();
    String log4jPropertyFile =
        conf.get(MRJobConfig.MAPREDUCE_JOB_LOG4J_PROPERTIES_FILE, "");
    if (!log4jPropertyFile.isEmpty()) {
      short replication = (short) conf.getInt(Job.SUBMIT_REPLICATION, 10);
      copyLog4jPropertyFile(job, jobSubmitDir, replication);
    }
  }



====================ctx:=======================
CtxCodeBlock{node=Node: < Application, Lorg/apache/hadoop/mapreduce/JobResourceUploader, uploadResourcesInternal(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V > Context: Everywhere, blocks=[BB[SSA:170..173]60 - org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V, BB[SSA:163..169]59 - org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V, BB[SSA:174..176]61 - org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V, BB[SSA:-1..-2]67 - org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/fs/Path;)V], numberOfBasicBlocks=4, firstLineNumber=211, lastLineNumber=217, firstMethodNumber=142, lastMethodNumber=227, isFirstLineValid=true, methodSrcCode=
      throws IOException {
    Configuration conf = job.getConfiguration();
    short replication =
        (short) conf.getInt(Job.SUBMIT_REPLICATION,
            Job.DEFAULT_SUBMIT_REPLICATION);

    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {
      LOG.warn("Hadoop command-line option parsing not performed. "
          + "Implement the Tool interface and execute your application "
          + "with ToolRunner to remedy this.");
    }

    //
    // Figure out what fs the JobTracker is using. Copy the
    // job to it, under a temporary name. This allows DFS to work,
    // and under the local fs also provides UNIX-like object loading
    // semantics. (that is, if the job file is deleted right after
    // submission, we can still run the submission to completion)
    //

    // Create a number of filenames in the JobTracker's fs namespace
    LOG.debug("default FileSystem: " + jtFs.getUri());
    if (jtFs.exists(submitJobDir)) {
      throw new IOException("Not submitting job. Job directory " + submitJobDir
          + " already exists!! This is unexpected.Please check what's there in"
          + " that directory");
    }
    // Create the submission directory for the MapReduce job.
    submitJobDir = jtFs.makeQualified(submitJobDir);
    submitJobDir = new Path(submitJobDir.toUri().getPath());
    FsPermission mapredSysPerms =
        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);
    mkdirs(jtFs, submitJobDir, mapredSysPerms);

    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,
        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {
      disableErasureCodingForPath(submitJobDir);
    }

    // Get the resources that have been added via command line arguments in the
    // GenericOptionsParser (i.e. files, libjars, archives).
    Collection<String> files = conf.getStringCollection("tmpfiles");
    Collection<String> libjars = conf.getStringCollection("tmpjars");
    Collection<String> archives = conf.getStringCollection("tmparchives");
    String jobJar = job.getJar();

    // Merge resources that have been programmatically specified for the shared
    // cache via the Job API.
    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));
    libjars.addAll(conf.getStringCollection(
            MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));
    archives.addAll(conf
        .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));


    Map<URI, FileStatus> statCache = new HashMap<URI, FileStatus>();
    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);

    Map<String, Boolean> fileSCUploadPolicies =
        new LinkedHashMap<String, Boolean>();
    Map<String, Boolean> archiveSCUploadPolicies =
        new LinkedHashMap<String, Boolean>();

    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,
        fileSCUploadPolicies, statCache);
    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,
        fileSCUploadPolicies, statCache);
    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,
        archiveSCUploadPolicies, statCache);
    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);
    addLog4jToDistributedCache(job, submitJobDir);

    // Note, we do not consider resources in the distributed cache for the
    // shared cache at this time. Only resources specified via the
    // GenericOptionsParser or the jobjar.
    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);
    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);

    // set the timestamps of the archives and files
    // set the public/private visibility of the archives and files
    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,
        statCache);
    // get DelegationToken for cached file
    ClientDistributedCacheManager.getDelegationTokens(conf,
        job.getCredentials());
  }

}
