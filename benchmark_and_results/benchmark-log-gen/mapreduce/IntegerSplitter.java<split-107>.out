====================TaintedSinkInfo:=======================
-------------TaintedSinkInfo----------
fileName:	IntegerSplitter.java	methodSinagture:	org.apache.hadoop.mapreduce.lib.db.IntegerSplitter.split(JJJ)Ljava/util/List;	methodLines:	107:130
blockLines:	120:-1
paras:	mapreduce.job.maps
TaintedStat:	NORMAL split:conditional branch(gt, to iindex=34) 14,12 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere
Source:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/DateSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere[38]36 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getInt(Ljava/lang/String;I)I > 2,34,8 @80 exception:35
Tainted Path:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/DateSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere[38]36 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getInt(Ljava/lang/String;I)I > 2,34,8 @80 exception:35
PHI Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/DateSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere:37 = phi  36,8
NORMAL split:42 = conversion(J) 37 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/DateSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere
PARAM_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/DateSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere[88]44 = invokevirtual < Application, Lorg/apache/hadoop/mapreduce/lib/db/DateSplitter, split(JJJ)Ljava/util/List; > 1,42,12,15 @185 exception:43 v42
PARAM_CALLEE:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere v2
NORMAL split:9 = binaryop(div) 8 , 2 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere
PHI Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere:13 = phi  9,10
NORMAL split:19 = binaryop(add) 20 , 13 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere
PHI Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere:20 = phi  19,3
NORMAL split:14 = compare 20,4 opcode=cmp Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere
NORMAL split:conditional branch(gt, to iindex=34) 14,12 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere

-------------TaintedSinkInfo----------
fileName:	IntegerSplitter.java	methodSinagture:	org.apache.hadoop.mapreduce.lib.db.IntegerSplitter.split(JJJ)Ljava/util/List;	methodLines:	107:130
blockLines:	114:-1
paras:	mapreduce.job.maps
TaintedStat:	NORMAL split:conditional branch(ge, to iindex=17) 11,12 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere
Source:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere[29]32 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getInt(Ljava/lang/String;I)I > 2,30,6 @64 exception:31
Tainted Path:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere[29]32 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getInt(Ljava/lang/String;I)I > 2,30,6 @64 exception:31
PHI Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere:33 = phi  32,6
NORMAL split:39 = conversion(J) 33 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere
PARAM_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere[79]41 = invokevirtual < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > 1,39,8,11 @171 exception:40 v39
PARAM_CALLEE:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere v2
NORMAL split:9 = binaryop(div) 8 , 2 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere
NORMAL split:11 = compare 9,10 opcode=cmp Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere
NORMAL split:conditional branch(ge, to iindex=17) 11,12 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(JJJ)Ljava/util/List; > Context: Everywhere



====================ExtendedBlocks:=======================


====================MethodSrc:=======================

    List<Long> splits = new ArrayList<Long>();

    // Use numSplits as a hint. May need an extra task if the size doesn't
    // divide cleanly.

    long splitSize = (maxVal - minVal) / numSplits;
    if (splitSize < 1) {
      splitSize = 1;
    }

    long curVal = minVal;

    while (curVal <= maxVal) {
      splits.add(curVal);
      curVal += splitSize;
    }

    if (splits.get(splits.size() - 1) != maxVal || splits.size() == 1) {
      // We didn't end on the maxVal. Add that to the end of the list.
      splits.add(maxVal);
    }

    return splits;
  }


====================ctx:=======================
CtxCodeBlock{node=Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/IntegerSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere, blocks=[BB[SSA:74..79]36 - org.apache.hadoop.mapreduce.lib.db.IntegerSplitter.split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List;, BB[SSA:39..40]17 - org.apache.hadoop.mapreduce.lib.db.IntegerSplitter.split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List;, BB[SSA:44..45]19 - org.apache.hadoop.mapreduce.lib.db.IntegerSplitter.split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List;, BB[SSA:80..81]37 - org.apache.hadoop.mapreduce.lib.db.IntegerSplitter.split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List;, BB[SSA:-1..-2]103 - org.apache.hadoop.mapreduce.lib.db.IntegerSplitter.split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List;], numberOfBasicBlocks=5, firstLineNumber=52, lastLineNumber=62, firstMethodNumber=40, lastMethodNumber=90, isFirstLineValid=true, methodSrcCode=

    long minVal = results.getLong(1);
    long maxVal = results.getLong(2);

    String lowClausePrefix = colName + " >= ";
    String highClausePrefix = colName + " < ";

    int numSplits = conf.getInt(MRJobConfig.NUM_MAPS, 1);
    if (numSplits < 1) {
      numSplits = 1;
    }

    if (results.getString(1) == null && results.getString(2) == null) {
      // Range is null to null. Return a null split accordingly.
      List<InputSplit> splits = new ArrayList<InputSplit>();
      splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(
          colName + " IS NULL", colName + " IS NULL"));
      return splits;
    }

    // Get all the split points together.
    List<Long> splitPoints = split(numSplits, minVal, maxVal);
    List<InputSplit> splits = new ArrayList<InputSplit>();

    // Turn the split points into a set of intervals.
    long start = splitPoints.get(0);
    for (int i = 1; i < splitPoints.size(); i++) {
      long end = splitPoints.get(i);

      if (i == splitPoints.size() - 1) {
        // This is the last one; use a closed interval.
        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(
            lowClausePrefix + Long.toString(start),
            colName + " <= " + Long.toString(end)));
      } else {
        // Normal open-interval case.
        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(
            lowClausePrefix + Long.toString(start),
            highClausePrefix + Long.toString(end)));
      }

      start = end;
    }

    if (results.getString(1) == null || results.getString(2) == null) {
      // At least one extrema is null; add a null split.
      splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(
          colName + " IS NULL", colName + " IS NULL"));
    }

    return splits;
  }
}
CtxCodeBlock{node=Node: < Application, Lorg/apache/hadoop/mapreduce/lib/db/DateSplitter, split(Lorg/apache/hadoop/conf/Configuration;Ljava/sql/ResultSet;Ljava/lang/String;)Ljava/util/List; > Context: Everywhere, blocks=null, numberOfBasicBlocks=0, firstLineNumber=0, lastLineNumber=0, firstMethodNumber=55, lastMethodNumber=126, isFirstLineValid=true, methodSrcCode=

    int sqlDataType = results.getMetaData().getColumnType(1);
    minVal = resultSetColToLong(results, 1, sqlDataType);
    maxVal = resultSetColToLong(results, 2, sqlDataType);

    String lowClausePrefix = colName + " >= ";
    String highClausePrefix = colName + " < ";

    int numSplits = conf.getInt(MRJobConfig.NUM_MAPS, 1);
    if (numSplits < 1) {
      numSplits = 1;
    }

    if (minVal == Long.MIN_VALUE && maxVal == Long.MIN_VALUE) {
      // The range of acceptable dates is NULL to NULL. Just create a single split.
      List<InputSplit> splits = new ArrayList<InputSplit>();
      splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(
          colName + " IS NULL", colName + " IS NULL"));
      return splits;
    }

    // Gather the split point integers
    List<Long> splitPoints = split(numSplits, minVal, maxVal);
    List<InputSplit> splits = new ArrayList<InputSplit>();

    // Turn the split points into a set of intervals.
    long start = splitPoints.get(0);
    Date startDate = longToDate(start, sqlDataType);
    if (sqlDataType == Types.TIMESTAMP) {
      // The lower bound's nanos value needs to match the actual lower-bound nanos.
      try {
        ((java.sql.Timestamp) startDate).setNanos(results.getTimestamp(1).getNanos());
      } catch (NullPointerException npe) {
        // If the lower bound was NULL, we'll get an NPE; just ignore it and don't set nanos.
      }
    }

    for (int i = 1; i < splitPoints.size(); i++) {
      long end = splitPoints.get(i);
      Date endDate = longToDate(end, sqlDataType);

      if (i == splitPoints.size() - 1) {
        if (sqlDataType == Types.TIMESTAMP) {
          // The upper bound's nanos value needs to match the actual upper-bound nanos.
          try {
            ((java.sql.Timestamp) endDate).setNanos(results.getTimestamp(2).getNanos());
          } catch (NullPointerException npe) {
            // If the upper bound was NULL, we'll get an NPE; just ignore it and don't set nanos.
          }
        }
        // This is the last one; use a closed interval.
        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(
            lowClausePrefix + dateToString(startDate),
            colName + " <= " + dateToString(endDate)));
      } else {
        // Normal open-interval case.
        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(
            lowClausePrefix + dateToString(startDate),
            highClausePrefix + dateToString(endDate)));
      }

      start = end;
      startDate = endDate;
    }

    if (minVal == Long.MIN_VALUE || maxVal == Long.MIN_VALUE) {
      // Add an extra split to handle the null case that we saw.
      splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(
          colName + " IS NULL", colName + " IS NULL"));
    }

    return splits;
  }
}
