====================TaintedSinkInfo:=======================
-------------TaintedSinkInfo----------
fileName:	MRAppMaster.java	methodSinagture:	org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster$Action;)Ljava/lang/Object;	methodLines:	1794:1808
blockLines:	1796:-1
paras:	null
TaintedStat:	NORMAL callWithJobClassLoader:conditional branch(eq, to iindex=13) 6,9 Node: < Application, Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster, callWithJobClassLoader(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster$Action;)Ljava/lang/Object; > Context: Everywhere
Source:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster, callWithJobClassLoader(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster$Action;)Ljava/lang/Object; > Context: Everywhere[1]6 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getClassLoader()Ljava/lang/ClassLoader; > 2 @1 exception:5
Tainted Path:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster, callWithJobClassLoader(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster$Action;)Ljava/lang/Object; > Context: Everywhere[1]6 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getClassLoader()Ljava/lang/ClassLoader; > 2 @1 exception:5
NORMAL callWithJobClassLoader:conditional branch(eq, to iindex=13) 6,9 Node: < Application, Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster, callWithJobClassLoader(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster$Action;)Ljava/lang/Object; > Context: Everywhere



====================ExtendedBlocks:=======================


====================MethodSrc:=======================
    // done
    ClassLoader currentClassLoader = conf.getClassLoader();
    boolean setJobClassLoader =
        jobClassLoader != null && currentClassLoader != jobClassLoader;
    if (setJobClassLoader) {
      MRApps.setClassLoader(jobClassLoader, conf);
    }
    try {
      return action.call(conf);
    } finally {
      if (setJobClassLoader) {
        // restore the original classloader
        MRApps.setClassLoader(currentClassLoader, conf);
      }
    }
  }


====================ctx:=======================
CtxCodeBlock{node=Node: < Application, Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster, serviceInit(Lorg/apache/hadoop/conf/Configuration;)V > Context: Everywhere, blocks=[BB[SSA:383..383]170 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(Lorg/apache/hadoop/conf/Configuration;)V, BB[SSA:380..382]169 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(Lorg/apache/hadoop/conf/Configuration;)V, BB[SSA:384..388]171 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(Lorg/apache/hadoop/conf/Configuration;)V, BB[SSA:-1..-2]241 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(Lorg/apache/hadoop/conf/Configuration;)V], numberOfBasicBlocks=4, firstLineNumber=431, lastLineNumber=440, firstMethodNumber=281, lastMethodNumber=495, isFirstLineValid=true, methodSrcCode=
    // create the job classloader if enabled
    createJobClassLoader(conf);

    initJobCredentialsAndUGI(conf);

    dispatcher = createDispatcher();
    addIfService(dispatcher);
    taskAttemptFinishingMonitor = createTaskAttemptFinishingMonitor(dispatcher.getEventHandler());
    addIfService(taskAttemptFinishingMonitor);
    context = new RunningAppContext(conf, taskAttemptFinishingMonitor);

    // Job name is the same as the app name util we support DAG of jobs
    // for an app later
    appName = conf.get(MRJobConfig.JOB_NAME, "<missing app name>");

    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());
     
    newApiCommitter = false;
    jobId = MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),
        appAttemptID.getApplicationId().getId());
    int numReduceTasks = conf.getInt(MRJobConfig.NUM_REDUCES, 0);
    if ((numReduceTasks > 0 && 
        conf.getBoolean("mapred.reducer.new-api", false)) ||
          (numReduceTasks == 0 && 
           conf.getBoolean("mapred.mapper.new-api", false)))  {
      newApiCommitter = true;
      LOG.info("Using mapred newApiCommitter.");
    }
    
    boolean copyHistory = false;
    committer = createOutputCommitter(conf);
    try {
      String user = UserGroupInformation.getCurrentUser().getShortUserName();
      Path stagingDir = MRApps.getStagingAreaDir(conf, user);
      FileSystem fs = getFileSystem(conf);

      boolean stagingExists = fs.exists(stagingDir);
      Path startCommitFile = MRApps.getStartJobCommitFile(conf, user, jobId);
      boolean commitStarted = fs.exists(startCommitFile);
      Path endCommitSuccessFile = MRApps.getEndJobCommitSuccessFile(conf, user, jobId);
      boolean commitSuccess = fs.exists(endCommitSuccessFile);
      Path endCommitFailureFile = MRApps.getEndJobCommitFailureFile(conf, user, jobId);
      boolean commitFailure = fs.exists(endCommitFailureFile);
      if(!stagingExists) {
        isLastAMRetry = true;
        LOG.info("Attempt num: " + appAttemptID.getAttemptId() +
            " is last retry: " + isLastAMRetry +
            " because the staging dir doesn't exist.");
        errorHappenedShutDown = true;
        forcedState = JobStateInternal.ERROR;
        shutDownMessage = "Staging dir does not exist " + stagingDir;
        LOG.error(shutDownMessage);
      } else if (commitStarted) {
        //A commit was started so this is the last time, we just need to know
        // what result we will use to notify, and how we will unregister
        errorHappenedShutDown = true;
        isLastAMRetry = true;
        LOG.info("Attempt num: " + appAttemptID.getAttemptId() +
            " is last retry: " + isLastAMRetry +
            " because a commit was started.");
        copyHistory = true;
        if (commitSuccess) {
          shutDownMessage =
              "Job commit succeeded in a prior MRAppMaster attempt " +
              "before it crashed. Recovering.";
          forcedState = JobStateInternal.SUCCEEDED;
        } else if (commitFailure) {
          shutDownMessage =
              "Job commit failed in a prior MRAppMaster attempt " +
              "before it crashed. Not retrying.";
          forcedState = JobStateInternal.FAILED;
        } else {
          if (isCommitJobRepeatable()) {
            // cleanup previous half done commits if committer supports
            // repeatable job commit.
            errorHappenedShutDown = false;
            cleanupInterruptedCommit(conf, fs, startCommitFile);
          } else {
            //The commit is still pending, commit error
            shutDownMessage =
                "Job commit from a prior MRAppMaster attempt is " +
                "potentially in progress. Preventing multiple commit executions";
            forcedState = JobStateInternal.ERROR;
          }
        }
      }
    } catch (IOException e) {
      throw new YarnRuntimeException("Error while initializing", e);
    }

    if (errorHappenedShutDown) {
      NoopEventHandler eater = new NoopEventHandler();
      //We do not have a JobEventDispatcher in this path
      dispatcher.register(JobEventType.class, eater);

      EventHandler<JobHistoryEvent> historyService = null;
      if (copyHistory) {
        historyService = 
          createJobHistoryHandler(context);
        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,
            historyService);
      } else {
        dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,
            eater);
      }

      if (copyHistory) {
        // Now that there's a FINISHING state for application on RM to give AMs
        // plenty of time to clean up after unregister it's safe to clean staging
        // directory after unregistering with RM. So, we start the staging-dir
        // cleaner BEFORE the ContainerAllocator so that on shut-down,
        // ContainerAllocator unregisters first and then the staging-dir cleaner
        // deletes staging directory.
        addService(createStagingDirCleaningService());
      }

      // service to allocate containers from RM (if non-uber) or to fake it (uber)
      containerAllocator = createContainerAllocator(null, context);
      addIfService(containerAllocator);
      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);

      if (copyHistory) {
        // Add the JobHistoryEventHandler last so that it is properly stopped first.
        // This will guarantee that all history-events are flushed before AM goes
        // ahead with shutdown.
        // Note: Even though JobHistoryEventHandler is started last, if any
        // component creates a JobHistoryEvent in the meanwhile, it will be just be
        // queued inside the JobHistoryEventHandler 
        addIfService(historyService);

        JobHistoryCopyService cpHist = new JobHistoryCopyService(appAttemptID,
            dispatcher.getEventHandler());
        addIfService(cpHist);
      }
    } else {

      //service to handle requests from JobClient
      clientService = createClientService(context);
      // Init ClientService separately so that we stop it separately, since this
      // service needs to wait some time before it stops so clients can know the
      // final states
      clientService.init(conf);
      
      containerAllocator = createContainerAllocator(clientService, context);
      
      //service to handle the output committer
      committerEventHandler = createCommitterEventHandler(context, committer);
      addIfService(committerEventHandler);

      //policy handling preemption requests from RM
      callWithJobClassLoader(conf, new Action<Void>() {
        public Void call(Configuration conf) {
          preemptionPolicy = createPreemptionPolicy(conf);
          preemptionPolicy.init(context);
          return null;
        }
      });

      //service to handle requests to TaskUmbilicalProtocol
      taskAttemptListener = createTaskAttemptListener(context, preemptionPolicy);
      addIfService(taskAttemptListener);

      //service to log job history events
      EventHandler<JobHistoryEvent> historyService = 
        createJobHistoryHandler(context);
      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,
          historyService);

      this.jobEventDispatcher = new JobEventDispatcher();

      //register the event dispatchers
      dispatcher.register(JobEventType.class, jobEventDispatcher);
      dispatcher.register(TaskEventType.class, new TaskEventDispatcher());
      dispatcher.register(TaskAttemptEventType.class, 
          new TaskAttemptEventDispatcher());
      dispatcher.register(CommitterEventType.class, committerEventHandler);

      if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)
          || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {
        //optional service to speculate on task attempts' progress
        speculator = createSpeculator(conf, context);
        addIfService(speculator);
      }

      speculatorEventDispatcher = new SpeculatorEventDispatcher(conf);
      dispatcher.register(Speculator.EventType.class,
          speculatorEventDispatcher);

      // Now that there's a FINISHING state for application on RM to give AMs
      // plenty of time to clean up after unregister it's safe to clean staging
      // directory after unregistering with RM. So, we start the staging-dir
      // cleaner BEFORE the ContainerAllocator so that on shut-down,
      // ContainerAllocator unregisters first and then the staging-dir cleaner
      // deletes staging directory.
      addService(createStagingDirCleaningService());

      // service to allocate containers from RM (if non-uber) or to fake it (uber)
      addIfService(containerAllocator);
      dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);

      // corresponding service to launch allocated containers via NodeManager
      containerLauncher = createContainerLauncher(context);
      addIfService(containerLauncher);
      dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);

      // Add the JobHistoryEventHandler last so that it is properly stopped first.
      // This will guarantee that all history-events are flushed before AM goes
      // ahead with shutdown.
      // Note: Even though JobHistoryEventHandler is started last, if any
      // component creates a JobHistoryEvent in the meanwhile, it will be just be
      // queued inside the JobHistoryEventHandler 
      addIfService(historyService);
    }
    super.serviceInit(conf);
  } // end of init()
  
}
CtxCodeBlock{node=Node: < Application, Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster, createSpeculator(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/AppContext;)Lorg/apache/hadoop/mapreduce/v2/app/speculate/Speculator; > Context: Everywhere, blocks=[BB[SSA:7..7]3 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createSpeculator(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/AppContext;)Lorg/apache/hadoop/mapreduce/v2/app/speculate/Speculator;, BB[SSA:3..6]2 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createSpeculator(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/AppContext;)Lorg/apache/hadoop/mapreduce/v2/app/speculate/Speculator;, BB[SSA:8..8]4 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createSpeculator(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/AppContext;)Lorg/apache/hadoop/mapreduce/v2/app/speculate/Speculator;, BB[SSA:-1..-2]6 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createSpeculator(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/mapreduce/v2/app/AppContext;)Lorg/apache/hadoop/mapreduce/v2/app/speculate/Speculator;], numberOfBasicBlocks=4, firstLineNumber=825, lastLineNumber=825, firstMethodNumber=824, lastMethodNumber=825, isFirstLineValid=true, methodSrcCode=
      final AppContext context) {
    return callWithJobClassLoader(conf, new Action<Speculator>() {
      public Speculator call(Configuration conf) {
}
CtxCodeBlock{node=Node: < Application, Lorg/apache/hadoop/mapreduce/v2/app/MRAppMaster, createOutputCommitter(Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/mapreduce/OutputCommitter; > Context: Everywhere, blocks=[BB[SSA:6..6]3 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/mapreduce/OutputCommitter;, BB[SSA:3..5]2 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/mapreduce/OutputCommitter;, BB[SSA:7..7]4 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/mapreduce/OutputCommitter;, BB[SSA:-1..-2]6 - org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/mapreduce/OutputCommitter;], numberOfBasicBlocks=4, firstLineNumber=534, lastLineNumber=534, firstMethodNumber=533, lastMethodNumber=534, isFirstLineValid=true, methodSrcCode=
  private OutputCommitter createOutputCommitter(Configuration conf) {
    return callWithJobClassLoader(conf, new Action<OutputCommitter>() {
      public OutputCommitter call(Configuration conf) {
}
