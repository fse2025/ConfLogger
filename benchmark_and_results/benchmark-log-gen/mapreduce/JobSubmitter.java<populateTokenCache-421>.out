====================TaintedSinkInfo:=======================
-------------TaintedSinkInfo----------
fileName:	JobSubmitter.java	methodSinagture:	org.apache.hadoop.mapreduce.JobSubmitter.populateTokenCache(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/security/Credentials;)V	methodLines:	421:434
blockLines:	427:-1
paras:	mapreduce.job.hdfs-servers
TaintedStat:	NORMAL populateTokenCache:conditional branch(eq, to iindex=50) 8,22 Node: < Application, Lorg/apache/hadoop/mapreduce/JobSubmitter, populateTokenCache(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/security/Credentials;)V > Context: Everywhere
Source:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/JobSubmitter, populateTokenCache(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/security/Credentials;)V > Context: Everywhere[6]8 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getStrings(Ljava/lang/String;)[Ljava/lang/String; > 2,6 @9 exception:7
Tainted Path:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/JobSubmitter, populateTokenCache(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/security/Credentials;)V > Context: Everywhere[6]8 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getStrings(Ljava/lang/String;)[Ljava/lang/String; > 2,6 @9 exception:7
NORMAL populateTokenCache:conditional branch(eq, to iindex=50) 8,22 Node: < Application, Lorg/apache/hadoop/mapreduce/JobSubmitter, populateTokenCache(Lorg/apache/hadoop/conf/Configuration;Lorg/apache/hadoop/security/Credentials;)V > Context: Everywhere



====================ExtendedBlocks:=======================


====================MethodSrc:=======================
  throws IOException{
    readTokensFromFiles(conf, credentials);
    // add the delegation tokens from configuration
    String [] nameNodes = conf.getStrings(MRJobConfig.JOB_NAMENODES);
    LOG.debug("adding the following namenodes' delegation tokens:" + 
        Arrays.toString(nameNodes));
    if(nameNodes != null) {
      Path [] ps = new Path[nameNodes.length];
      for(int i=0; i< nameNodes.length; i++) {
        ps[i] = new Path(nameNodes[i]);
      }
      TokenCache.obtainTokensForNamenodes(credentials, ps, conf);
    }
  }



====================ctx:=======================
CtxCodeBlock{node=Node: < Application, Lorg/apache/hadoop/mapreduce/JobSubmitter, submitJobInternal(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/mapreduce/Cluster;)Lorg/apache/hadoop/mapreduce/JobStatus; > Context: Everywhere, blocks=[BB[SSA:95..95]41 - org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/mapreduce/Cluster;)Lorg/apache/hadoop/mapreduce/JobStatus;, BB[SSA:91..94]40 - org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/mapreduce/Cluster;)Lorg/apache/hadoop/mapreduce/JobStatus;, BB[SSA:96..97]42 - org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(Lorg/apache/hadoop/mapreduce/Job;Lorg/apache/hadoop/mapreduce/Cluster;)Lorg/apache/hadoop/mapreduce/JobStatus;], numberOfBasicBlocks=3, firstLineNumber=173, lastLineNumber=176, firstMethodNumber=142, lastMethodNumber=265, isFirstLineValid=true, methodSrcCode=
    //validate the jobs output specs 
    checkSpecs(job);

    Configuration conf = job.getConfiguration();
    addMRFrameworkToDistributedCache(conf);

    Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);
    //configure the command line options correctly on the submitting dfs
    InetAddress ip = InetAddress.getLocalHost();
    if (ip != null) {
      submitHostAddress = ip.getHostAddress();
      submitHostName = ip.getHostName();
      conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);
      conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);
    }
    JobID jobId = submitClient.getNewJobID();
    job.setJobID(jobId);
    Path submitJobDir = new Path(jobStagingArea, jobId.toString());
    JobStatus status = null;
    try {
      conf.set(MRJobConfig.USER_NAME,
          UserGroupInformation.getCurrentUser().getShortUserName());
      conf.set("hadoop.http.filter.initializers", 
          "org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer");
      conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());
      LOG.debug("Configuring job " + jobId + " with " + submitJobDir 
          + " as the submit dir");
      // get delegation token for the dir
      TokenCache.obtainTokensForNamenodes(job.getCredentials(),
          new Path[] { submitJobDir }, conf);
      
      populateTokenCache(conf, job.getCredentials());

      // generate a secret to authenticate shuffle transfers
      if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {
        KeyGenerator keyGen;
        try {
          keyGen = KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);
          keyGen.init(SHUFFLE_KEY_LENGTH);
        } catch (NoSuchAlgorithmException e) {
          throw new IOException("Error generating shuffle secret key", e);
        }
        SecretKey shuffleKey = keyGen.generateKey();
        TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),
            job.getCredentials());
      }
      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
        conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);
        LOG.warn("Max job attempts set to 1 since encrypted intermediate" +
                "data spill is enabled");
      }

      copyAndConfigureFiles(job, submitJobDir);

      Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);
      
      // Create the splits for the job
      LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));
      int maps = writeSplits(job, submitJobDir);
      conf.setInt(MRJobConfig.NUM_MAPS, maps);
      LOG.info("number of splits:" + maps);

      int maxMaps = conf.getInt(MRJobConfig.JOB_MAX_MAP,
          MRJobConfig.DEFAULT_JOB_MAX_MAP);
      if (maxMaps >= 0 && maxMaps < maps) {
        throw new IllegalArgumentException("The number of map tasks " + maps +
            " exceeded limit " + maxMaps);
      }

      // write "queue admins of the queue to which job is being submitted"
      // to job file.
      String queue = conf.get(MRJobConfig.QUEUE_NAME,
          JobConf.DEFAULT_QUEUE_NAME);
      AccessControlList acl = submitClient.getQueueAdmins(queue);
      conf.set(toFullPropertyName(queue,
          QueueACL.ADMINISTER_JOBS.getAclName()), acl.getAclString());

      // removing jobtoken referrals before copying the jobconf to HDFS
      // as the tasks don't need this setting, actually they may break
      // because of it if present as the referral will point to a
      // different job.
      TokenCache.cleanUpTokenReferral(conf);

      if (conf.getBoolean(
          MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,
          MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {
        // Add HDFS tracking ids
        ArrayList<String> trackingIds = new ArrayList<String>();
        for (Token<? extends TokenIdentifier> t :
            job.getCredentials().getAllTokens()) {
          trackingIds.add(t.decodeIdentifier().getTrackingId());
        }
        conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,
            trackingIds.toArray(new String[trackingIds.size()]));
      }

      // Set reservation info if it exists
      ReservationId reservationId = job.getReservationId();
      if (reservationId != null) {
        conf.set(MRJobConfig.RESERVATION_ID, reservationId.toString());
      }

      // Write job file to submit dir
      writeConf(conf, submitJobFile);
      
      //
      // Now, actually submit the job (using the submit name)
      //
      printTokens(jobId, job.getCredentials());
      status = submitClient.submitJob(
          jobId, submitJobDir.toString(), job.getCredentials());
      if (status != null) {
        return status;
      } else {
        throw new IOException("Could not launch job");
      }
    } finally {
      if (status == null) {
        LOG.info("Cleaning up the staging area " + submitJobDir);
        if (jtFs != null && submitJobDir != null)
          jtFs.delete(submitJobDir, true);

      }
    }
  }
}
