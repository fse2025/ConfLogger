====================TaintedSinkInfo:=======================
-------------TaintedSinkInfo----------
fileName:	TotalOrderPartitioner.java	methodSinagture:	org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner.setConf(Lorg/apache/hadoop/conf/Configuration;)V	methodLines:	79:119
blockLines:	102:-1
paras:	null
TaintedStat:	NORMAL setConf:conditional branch(eq, to iindex=111) 41,11 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner, setConf(Lorg/apache/hadoop/conf/Configuration;)V > Context: Everywhere
Source:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner, setConf(Lorg/apache/hadoop/conf/Configuration;)V > Context: Everywhere[84]41 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getBoolean(Ljava/lang/String;Z)Z > 2,39,26 @152 exception:40
Tainted Path:	NORMAL_RET_CALLER:Node: < Application, Lorg/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner, setConf(Lorg/apache/hadoop/conf/Configuration;)V > Context: Everywhere[84]41 = invokevirtual < Application, Lorg/apache/hadoop/conf/Configuration, getBoolean(Ljava/lang/String;Z)Z > 2,39,26 @152 exception:40
NORMAL setConf:conditional branch(eq, to iindex=111) 41,11 Node: < Application, Lorg/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner, setConf(Lorg/apache/hadoop/conf/Configuration;)V > Context: Everywhere



====================ExtendedBlocks:=======================


====================MethodSrc:=======================
    try {
      this.conf = conf;
      String parts = getPartitionFile(conf);
      final Path partFile = new Path(parts);
      final FileSystem fs = (DEFAULT_PATH.equals(parts))
        ? FileSystem.getLocal(conf)     // assume in DistributedCache
        : partFile.getFileSystem(conf);

      Job job = Job.getInstance(conf);
      Class<K> keyClass = (Class<K>)job.getMapOutputKeyClass();
      K[] splitPoints = readPartitions(fs, partFile, keyClass, conf);
      if (splitPoints.length != job.getNumReduceTasks() - 1) {
        throw new IOException("Wrong number of partitions in keyset");
      }
      RawComparator<K> comparator =
        (RawComparator<K>) job.getSortComparator();
      for (int i = 0; i < splitPoints.length - 1; ++i) {
        if (comparator.compare(splitPoints[i], splitPoints[i+1]) >= 0) {
          throw new IOException("Split points are out of order");
        }
      }
      boolean natOrder =
        conf.getBoolean(NATURAL_ORDER, true);
      if (natOrder && BinaryComparable.class.isAssignableFrom(keyClass)) {
        partitions = buildTrie((BinaryComparable[])splitPoints, 0,
            splitPoints.length, new byte[0],
            // Now that blocks of identical splitless trie nodes are 
            // represented reentrantly, and we develop a leaf for any trie
            // node with only one split point, the only reason for a depth
            // limit is to refute stack overflow or bloat in the pathological
            // case where the split points are long and mostly look like bytes 
            // iii...iixii...iii   .  Therefore, we make the default depth
            // limit large but not huge.
            conf.getInt(MAX_TRIE_DEPTH, 200));
      } else {
        partitions = new BinarySearchNode(splitPoints, comparator);
      }
    } catch (IOException e) {
      throw new IllegalArgumentException("Can't read partitions file", e);
    }
  }



====================ctx:=======================
CtxCodeBlock{node=Node: < Application, Lorg/apache/hadoop/mapred/lib/TotalOrderPartitioner, configure(Lorg/apache/hadoop/mapred/JobConf;)V > Context: Everywhere, blocks=[BB[SSA:0..2]1 - org.apache.hadoop.mapred.lib.TotalOrderPartitioner.configure(Lorg/apache/hadoop/mapred/JobConf;)V, BB[SSA:-1..-2]0 - org.apache.hadoop.mapred.lib.TotalOrderPartitioner.configure(Lorg/apache/hadoop/mapred/JobConf;)V, BB[SSA:3..3]2 - org.apache.hadoop.mapred.lib.TotalOrderPartitioner.configure(Lorg/apache/hadoop/mapred/JobConf;)V, BB[SSA:-1..-2]3 - org.apache.hadoop.mapred.lib.TotalOrderPartitioner.configure(Lorg/apache/hadoop/mapred/JobConf;)V], numberOfBasicBlocks=4, firstLineNumber=40, lastLineNumber=42, firstMethodNumber=40, lastMethodNumber=42, isFirstLineValid=false, methodSrcCode=
  public void configure(JobConf job) {
    super.setConf(job);
  }

}
